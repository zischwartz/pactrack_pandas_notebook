{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "require([\"codemirror/keymap/sublime\", \"notebook/js/cell\", \"base/js/namespace\"],\n",
       "    function(sublime_keymap, cell, IPython) {\n",
       "        cell.Cell.options_default.cm_config.keyMap = 'sublime';\n",
       "        var cells = IPython.notebook.get_cells();\n",
       "        for(var cl=0; cl< cells.length ; cl++){\n",
       "            cells[cl].code_mirror.setOption('keyMap', 'sublime');\n",
       "        }\n",
       "    }\n",
       ");"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "import my_keymap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage, to get the file:\n",
    "# https://www.fec.gov/files/bulk-downloads/2018/cn18.zip\n",
    "# as a dataframe\n",
    "# cands = get_df('cn', [2018])\n",
    "\n",
    "# accepts: 'cm','oth','pas2','cn','webk','ccl'\n",
    "\n",
    "# pacs = get_df('webk', [2018], True)\n",
    "# tra = get_df('webk', [2018], True)\n",
    "# pacs\n",
    "# pac_to_cand_transactions =  get_df('pas2', [2018], True)\n",
    "# pac_to_cand_transactions =  get_df('pas2', [2018], False)\n",
    "# pac_to_pac_transactions = get_df('oth', [2018])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is meant to be imported as a python module, into a notebook, to make it easer to load fec data\n",
    "\n",
    "# XXX\n",
    "# XXX WARNING, TO IMPORT THIS, IT NEEDS TO BY A .py FILE, NOT AN .ipynb FILE. \n",
    "# Convert it in the container using `yarn run nbconvert`\n",
    "# XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX there's no header file for webk ! \n",
    "# generated via js at: \n",
    "# https://www.fec.gov/campaign-finance-data/pac-and-party-summary-file-description/\n",
    "# js\n",
    "# Array.from(document.querySelectorAll('tr+ tr td:nth-child(1)')).map( x=> x.innerHTML)\n",
    "webk_column_names = [\"CMTE_ID\", \"CMTE_NM\", \"CMTE_TP\", \"CMTE_DSGN\", \"CMTE_FILING_FREQ\", \"TTL_RECEIPTS\", \"TRANS_FROM_AFF\", \"INDV_CONTRIB\", \"OTHER_POL_CMTE_CONTRIB\", \"CAND_CONTRIB\", \" CAND_LOANS\", \"TTL_LOANS_RECEIVED\", \"TTL_DISB\", \"TRANF_TO_AFF\", \"INDV_REFUNDS\", \"OTHER_POL_CMTE_REFUNDS\", \"CAND_LOAN_REPAY\", \"LOAN_REPAY\", \"COH_BOP\", \"COH_COP\", \"DEBTS_OWED_BY\", \"NONFED_TRANS_RECEIVED\", \"CONTRIB_TO_OTHER_CMTE\", \"IND_EXP\", \"PTY_COORD_EXP\", \"NONFED_SHARE_EXP\", \"CVG_END_DT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the zip and header files for a type of record, given an array of years\n",
    "def get_df(name, years=[2018], apply_amendments=False, force_data_download=False):\n",
    "    # where we'll store the final result\n",
    "    result_df = pd.DataFrame()\n",
    "    # get the header file\n",
    "    header_url = \"https://www.fec.gov/data/advanced/files/bulk-downloads/data_dictionaries/{}_header_file.csv\".format(name)\n",
    "    header_file_name = \"{}_header_file.csv\".format(name)\n",
    "    if not os.path.isfile(header_file_name):\n",
    "        ! curl -L $header_url > $header_file_name\n",
    "    head = pd.read_csv(header_file_name, header=None)\n",
    "    names = head.values.tolist()[0]\n",
    "    if name is 'webk':\n",
    "        names = webk_column_names\n",
    "    # the default\n",
    "    the_dtypes = {}    \n",
    "    # is it transactions? if so, make them a string, so we can parse the dates for amendments     \n",
    "    if name in ['pas2', 'oth']:\n",
    "        the_dtypes = {'TRANSACTION_DT': 'str'}\n",
    "\n",
    "    # get the actual data \n",
    "    for year in years:\n",
    "        # get the last two digts\n",
    "        year_str = str(year)[-2:]\n",
    "        # zip_file_name = \"{}{}.zip\".format(name, year%100)\n",
    "        zip_file_name = \"{}{}.zip\".format(name, year_str)        \n",
    "        zip_url = \"https://www.fec.gov/files/bulk-downloads/{}/{}\".format(year, zip_file_name)\n",
    "        print('loading:', zip_file_name)\n",
    "        if not os.path.isfile(zip_file_name) or force_data_download:\n",
    "            print('downloading from: ', zip_url)\n",
    "            ! curl -L $zip_url > $zip_file_name\n",
    "        df = pd.read_csv(zip_file_name, compression='zip', sep='|', header=None, names=names, low_memory=False, dtype=the_dtypes, na_values='')\n",
    "\n",
    "        # combine them\n",
    "        result_df = pd.concat([df, result_df], ignore_index=True)\n",
    "    # do we need to deal with dates and amendments, XXX maybe do this above, before combining years, might be more performant?\n",
    "    if apply_amendments and name in ['pas2', 'oth']:\n",
    "        # print('before', len(result_df))        \n",
    "        # parse the dates \n",
    "        result_df['TRANSACTION_DT'] = pd.to_datetime(result_df['TRANSACTION_DT'], format='%m%d%Y',  errors = 'coerce') # errors = 'ignore'\n",
    "        # group them by transaction id and apply the amendment, i.e. use the most recent\n",
    "        # https://stackoverflow.com/a/41525937/83859\n",
    "        result_df = result_df.loc[result_df.groupby('TRAN_ID')['TRANSACTION_DT'].idxmax()]\n",
    "        # print('after', len(result_df))\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  df = pd.read_csv(zip_file_name, compression='zip', sep='|', header=None, names=names, low_memory=False, date_parser=the_date_parser, parse_dates=date_cols, dtype=the_dtypes)\n",
    " \n",
    "# print('You forgot to comment out your testing in download_data.ipynb. Fix it and run nbconvert')\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n",
    "# fec_dateparse = lambda x: pd.datetime.strptime(x, '%m%d%Y')\n",
    "# fec_dateparse('01011981')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
